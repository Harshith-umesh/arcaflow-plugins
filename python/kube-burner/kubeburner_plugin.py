#!/usr/bin/env python3

import re
import sys
import typing
from dataclasses import dataclass, field
from typing import List,Dict
from arcaflow_plugin_sdk import plugin,schema
import subprocess
import datetime
import yaml


@dataclass
class KubeBurnerIndexerInputParams:
    """
    This is the data structure for the input parameters for kube-burner indexer.
    """
    collection_time: int = field(metadata={"name": "Time", "description": "Duration for which to collect the prometheus metrics"})
    es_server: str = field(default="https://search-perfscale-dev-chmf5l4sh66lvxbnadi4bznl3a.us-west-2.es.amazonaws.com:443",metadata={"name": "Elasticsearch server url", "description": "URL for your elasticsearch endpoint"})
    es_index: str = field(default="ripsaw-kube-burner",metadata={"name": "Elasticsearch index name", "description": "Elasticsearch index to use for indexing the documents"})

@dataclass
class KubeBurnerIndexerOutput:
    """
    This is the data structure for output returned by the kube-burner indexer.
    """
    uuid: str = field(metadata={"name": "UUID", "description": "UUID generated for this workload run"})
    output: str = field(metadata={"name": "Kube burner workload output", "description": "Output generated by the kube burner workload"})


@dataclass
class WorkloadError:
    """
    This is the output data structure in the error case.
    """
    exit_code: int = field(metadata={
        "name": "Exit Code", "description": "Exit code returned by the program in case of a failure"})
    error: str = field(metadata={
        "name": "Failure Error", "description": "Reason for failure"})

kube_burner_indexer_input_schema = plugin.build_object_schema(KubeBurnerIndexerInputParams)
kube_burner_indexer_output_schema = plugin.build_object_schema(KubeBurnerIndexerOutput)

def uuidgen():
    cmd="uuidgen"
    uuid = subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True)
    uuid = uuid.decode("utf-8")
    uuid=uuid.strip()
    return uuid

def getprometheuscreds():
    cmd='oc get route -n openshift-monitoring prometheus-k8s -o jsonpath="{.spec.host}"'
    prom_url= subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True)
    prom_url = prom_url.decode("utf-8")
    prom_url="https://"+prom_url

    cmd='oc -n openshift-monitoring sa get-token prometheus-k8s'
    prom_token= subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True)
    prom_token = prom_token.decode("utf-8")

    return prom_url,prom_token


@plugin.step(
    id="kubeburnerindexer",
    name="Kube-Burner Indexer Workload",
    description="Collect and index Prometheus metrics for a specified time period",
    outputs={"success": KubeBurnerIndexerOutput, "error": WorkloadError},
)
def RunKubeBurnerIndexer(params: KubeBurnerIndexerInputParams ) -> typing.Tuple[str, typing.Union[KubeBurnerIndexerOutput, WorkloadError]]:

    print("==>> Running Kube Burner Indexer to collect metrics over the last {} minutes ...".format(params.collection_time))
    
    try:
        with open("configs/kubeburner_indexer.yml", "r") as input:
            try:
                config = yaml.safe_load(input)
            except yaml.YAMLError as error:
                return "error", WorkloadError(f"{error} reading kubeburner_indexer.yml")
    except EnvironmentError as error:
            return "error", WorkloadError(f"{error} while trying to open kubeburner_indexer.yml")

    config['global']['indexerConfig']['esServers'].append(params.es_server) 
    config['global']['indexerConfig']['defaultIndex'] = params.es_index

    with open('configs/kubeburner_indexer.yml', 'w') as yaml_file:
        yaml_file.write( yaml.dump(config, default_flow_style=False))



    uuid = uuidgen()
    prom_url , prom_token = getprometheuscreds()

    current_time = datetime.datetime.now() 
    time_duration = current_time - datetime.timedelta(minutes=params.collection_time)
    start_ts = str(int(time_duration.timestamp()))
    current_ts= str(int(datetime.datetime.now().timestamp()))

    try:
        cmd=['./kube-burner', 'index', '-c','configs/kubeburner_indexer.yml', '--uuid='+str(uuid), '-u='+str(prom_url), '--job-name', 'kube-burner-indexer', '--token='+str(prom_token), '-m=configs/metrics.yaml', '--start',start_ts, '--end', current_ts]
        process_out = subprocess.check_output(cmd, stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as error:
        return "error", WorkloadError(error.returncode,"{} failed with return code {}:\n{}".format(error.cmd[0],error.returncode,error.output))

    output = process_out.decode("utf-8")


    print("==>> Kube Burner Indexing complete!")
    
    return "success", KubeBurnerIndexerOutput(uuid,output)
    



if __name__ == "__main__":
    sys.exit(plugin.run(plugin.build_schema(
        RunKubeBurnerIndexer
    )))
